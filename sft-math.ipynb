{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:51:01.188972Z",
     "iopub.status.busy": "2025-11-21T19:51:01.188266Z",
     "iopub.status.idle": "2025-11-21T19:52:12.468067Z",
     "shell.execute_reply": "2025-11-21T19:52:12.467189Z",
     "shell.execute_reply.started": "2025-11-21T19:51:01.188933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T19:52:30.419165Z",
     "iopub.status.busy": "2025-11-21T19:52:30.418683Z",
     "iopub.status.idle": "2025-11-21T19:53:03.239767Z",
     "shell.execute_reply": "2025-11-21T19:53:03.239169Z",
     "shell.execute_reply.started": "2025-11-21T19:52:30.419127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:53:08.055392Z",
     "iopub.status.busy": "2025-11-21T19:53:08.054234Z",
     "iopub.status.idle": "2025-11-21T19:53:08.059074Z",
     "shell.execute_reply": "2025-11-21T19:53:08.058379Z",
     "shell.execute_reply.started": "2025-11-21T19:53:08.055351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Константы\n",
    "DATASET_NAME = 'nlile/hendrycks-MATH-benchmark'\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-Math-1.5B-Instruct'\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:53:10.236384Z",
     "iopub.status.busy": "2025-11-21T19:53:10.235632Z",
     "iopub.status.idle": "2025-11-21T19:53:15.861355Z",
     "shell.execute_reply": "2025-11-21T19:53:15.860791Z",
     "shell.execute_reply.started": "2025-11-21T19:53:10.236359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем датасет\n",
    "data = load_dataset(DATASET_NAME)\n",
    "data = data.remove_columns(['subject', 'level', 'unique_id'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:53:18.791223Z",
     "iopub.status.busy": "2025-11-21T19:53:18.790410Z",
     "iopub.status.idle": "2025-11-21T19:53:45.958710Z",
     "shell.execute_reply": "2025-11-21T19:53:45.957910Z",
     "shell.execute_reply.started": "2025-11-21T19:53:18.791199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем модель и токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype='auto', device_map='auto', load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:53:49.107223Z",
     "iopub.status.busy": "2025-11-21T19:53:49.106466Z",
     "iopub.status.idle": "2025-11-21T19:53:49.233784Z",
     "shell.execute_reply": "2025-11-21T19:53:49.233179Z",
     "shell.execute_reply.started": "2025-11-21T19:53:49.107185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Определяем LoRA адаптер\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:53:51.199302Z",
     "iopub.status.busy": "2025-11-21T19:53:51.199021Z",
     "iopub.status.idle": "2025-11-21T19:53:56.799160Z",
     "shell.execute_reply": "2025-11-21T19:53:56.798185Z",
     "shell.execute_reply.started": "2025-11-21T19:53:51.199282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Определяем системный промпт\n",
    "SYSTEM_PROMPT = 'You are an AI assistant skilled in mathematical reasoning. Please solve the problem using concise step-by-step reasoning process.'\n",
    "SYSTEM_PROMPT += ' Put your final answer within \\\\boxed{}.'\n",
    "\n",
    "# Функция для токенизации и подготовки текста\n",
    "def preprocess_function(example):\n",
    "    problem = example['problem']\n",
    "    solution = example['solution']\n",
    "    answer = example['answer']\n",
    "\n",
    "    # Формируем полный промпт\n",
    "    answer = str(answer).strip()\n",
    "    prompt = f'<|system|>\\n{SYSTEM_PROMPT}\\n<|user|>\\n{problem}\\n<|assistant|>\\n'\n",
    "    if '\\\\boxed{' in solution:\n",
    "        full_text = prompt + solution\n",
    "    else:\n",
    "        full_text = prompt + solution + f'The final answer is \\\\boxed{{{answer}}}'\n",
    "\n",
    "    # Токенизируем полный промпт\n",
    "    full_text_tokenized = tokenizer(\n",
    "        full_text,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    # Токенизиурем только промпт без решения\n",
    "    prompt_tokenized = tokenizer(\n",
    "        prompt,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    input_ids = full_text_tokenized['input_ids']\n",
    "    attention_mask = full_text_tokenized['attention_mask']\n",
    "\n",
    "    # Маскируем часть промпта которая не содержит решения\n",
    "    labels = input_ids.copy()\n",
    "    prompt_length = len(prompt_tokenized['input_ids'])\n",
    "    if prompt_length < len(labels):\n",
    "        labels[:prompt_length] = [-100] * prompt_length\n",
    "    else:\n",
    "        labels = [-100] * len(labels)\n",
    "\n",
    "    # Возвращаем результат\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:54:01.889349Z",
     "iopub.status.busy": "2025-11-21T19:54:01.888667Z",
     "iopub.status.idle": "2025-11-21T19:54:18.412696Z",
     "shell.execute_reply": "2025-11-21T19:54:18.412064Z",
     "shell.execute_reply.started": "2025-11-21T19:54:01.889315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Токенизируем задачи и делаем разделение на train и eval\n",
    "tokenized_dataset = data.map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    desc='Tokenize dataset',\n",
    "    remove_columns=data['train'].column_names\n",
    ")\n",
    "split = tokenized_dataset['train'].train_test_split(test_size=0.2, seed=RANDOM_STATE)\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'eval': split['test'],\n",
    "    'test': tokenized_dataset['test']\n",
    "})\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:54:21.306681Z",
     "iopub.status.busy": "2025-11-21T19:54:21.306164Z",
     "iopub.status.idle": "2025-11-21T19:54:21.313199Z",
     "shell.execute_reply": "2025-11-21T19:54:21.312478Z",
     "shell.execute_reply.started": "2025-11-21T19:54:21.306660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Распаковываем в отдельные списки\n",
    "        input_ids = [f['input_ids'] for f in features]\n",
    "        attention_mask = [f['attention_mask'] for f in features]\n",
    "        labels = [f['labels'] for f in features]\n",
    "\n",
    "        # Определяем максимальную длину в батче\n",
    "        max_len = min(max([len(i) for i in input_ids]), self.max_length)\n",
    "        \n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        padded_labels = []\n",
    "\n",
    "        for ids, mask, label in zip(input_ids, attention_mask, labels):\n",
    "            # Усекаем последовательности\n",
    "            ids = ids[:max_len]\n",
    "            mask = mask[:max_len]\n",
    "            label = mask[:max_len]\n",
    "\n",
    "            # Добавляем паддинг\n",
    "            pad_length = max_len - len(ids)\n",
    "            if pad_length > 0:\n",
    "                ids += [self.tokenizer.pad_token_id] * pad_length\n",
    "                mask += [0] * pad_length\n",
    "                label += [-100] * pad_length\n",
    "\n",
    "            padded_input_ids.append(ids)\n",
    "            padded_attention_mask.append(mask)\n",
    "            padded_labels.append(label)\n",
    "        \n",
    "        # Возвращаем результат\n",
    "        return {\n",
    "            'input_ids': torch.tensor(padded_input_ids).long(),\n",
    "            'attention_mask': torch.tensor(padded_attention_mask).long(),\n",
    "            'labels': torch.tensor(padded_labels).long()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:54:26.986936Z",
     "iopub.status.busy": "2025-11-21T19:54:26.986306Z",
     "iopub.status.idle": "2025-11-21T19:54:26.990415Z",
     "shell.execute_reply": "2025-11-21T19:54:26.989639Z",
     "shell.execute_reply.started": "2025-11-21T19:54:26.986909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:54:39.481190Z",
     "iopub.status.busy": "2025-11-21T19:54:39.480347Z",
     "iopub.status.idle": "2025-11-21T19:54:39.509487Z",
     "shell.execute_reply": "2025-11-21T19:54:39.508754Z",
     "shell.execute_reply.started": "2025-11-21T19:54:39.481163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "    label_names=['labels'],\n",
    "\n",
    "    report_to='none',\n",
    "    \n",
    "    do_eval=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    \n",
    "    save_strategy='no',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T19:54:43.277779Z",
     "iopub.status.busy": "2025-11-21T19:54:43.276970Z",
     "iopub.status.idle": "2025-11-21T19:54:43.296765Z",
     "shell.execute_reply": "2025-11-21T19:54:43.296011Z",
     "shell.execute_reply.started": "2025-11-21T19:54:43.277738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['eval'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-21T20:01:42.648Z",
     "iopub.execute_input": "2025-11-21T19:54:48.134938Z",
     "iopub.status.busy": "2025-11-21T19:54:48.134399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Обучение\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем веса LoRA\n",
    "trainer.model.save_pretrained('lora_adapter')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
