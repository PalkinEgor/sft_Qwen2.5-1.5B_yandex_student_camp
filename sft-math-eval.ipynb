{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T20:17:54.239540Z",
     "iopub.status.busy": "2025-11-23T20:17:54.238740Z",
     "iopub.status.idle": "2025-11-23T20:18:24.970785Z",
     "shell.execute_reply": "2025-11-23T20:18:24.970218Z",
     "shell.execute_reply.started": "2025-11-23T20:17:54.239512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:18:28.764535Z",
     "iopub.status.busy": "2025-11-23T20:18:28.763760Z",
     "iopub.status.idle": "2025-11-23T20:18:28.769166Z",
     "shell.execute_reply": "2025-11-23T20:18:28.768111Z",
     "shell.execute_reply.started": "2025-11-23T20:18:28.764509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Константы\n",
    "DATASET_NAME = 'nlile/hendrycks-MATH-benchmark'\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-Math-1.5B-Instruct'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 16\n",
    "MAX_NEW_TOKENS = 1024\n",
    "LORA = False\n",
    "LORA_PATH = 'lora_adapter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T20:18:32.312924Z",
     "iopub.status.busy": "2025-11-23T20:18:32.312021Z",
     "iopub.status.idle": "2025-11-23T20:18:35.197311Z",
     "shell.execute_reply": "2025-11-23T20:18:35.196736Z",
     "shell.execute_reply.started": "2025-11-23T20:18:32.312896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем датасет\n",
    "data = load_dataset(DATASET_NAME)\n",
    "data = data.remove_columns(['subject', 'level', 'unique_id'])['test']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:19:01.514741Z",
     "iopub.status.busy": "2025-11-23T17:19:01.514048Z",
     "iopub.status.idle": "2025-11-23T17:19:40.104626Z",
     "shell.execute_reply": "2025-11-23T17:19:40.103751Z",
     "shell.execute_reply.started": "2025-11-23T17:19:01.514706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем модель и токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype='auto', device_map='auto')\n",
    "if LORA:\n",
    "    model = PeftModel.from_pretrained(model, LORA_PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:19:46.900971Z",
     "iopub.status.busy": "2025-11-23T17:19:46.899808Z",
     "iopub.status.idle": "2025-11-23T17:19:46.906720Z",
     "shell.execute_reply": "2025-11-23T17:19:46.905916Z",
     "shell.execute_reply.started": "2025-11-23T17:19:46.900936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Определяем системный промпт\n",
    "SYSTEM_PROMPT = 'You are an AI assistant skilled in mathematical reasoning. Please solve the problem using concise step-by-step reasoning process.'\n",
    "SYSTEM_PROMPT += ' Put your final answer within \\\\boxed{}.'\n",
    "\n",
    "# Конструктор промпта\n",
    "def build_prompt(problem):\n",
    "    return f'<|system|>\\n{SYSTEM_PROMPT}\\n<|user|>\\n{problem}\\n<|assistant|>\\n'\n",
    "\n",
    "# Подготовка батча\n",
    "def preprocess_batch(batch, max_length=1024):\n",
    "    prompts = [build_prompt(p) for p in batch['problem']]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Извлечение ответа\n",
    "def extract_answer(answer):\n",
    "    box_pattern = r'\\\\boxed\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}'\n",
    "    box_match = re.search(box_pattern, answer)\n",
    "    if box_match:\n",
    "        return box_match.group(1).strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Вычисление метрики pass@k\n",
    "def pass_at_k(preds, gold, k):\n",
    "    correct = 0\n",
    "    for pred, g in zip(preds, gold):\n",
    "        correct += any(extract_answer(p) == extract_answer(g) for p in pred[:k])\n",
    "    return correct / len(gold)\n",
    "\n",
    "# Вычисление метрики self-consistency\n",
    "def self_consistency(preds):\n",
    "    result = []\n",
    "    for pred in preds:\n",
    "        answers = [extract_answer(p) for p in pred]\n",
    "        if answers == [''] * len(answers):\n",
    "            result.append(0)\n",
    "            continue\n",
    "        most_common = {}\n",
    "        for answer in answers:\n",
    "            most_common[answer] = most_common.get(answer, 0) + 1\n",
    "        most_common = max(most_common, key=most_common.get)\n",
    "        result.append(answers.count(most_common) / len(answers))\n",
    "    return sum(result) / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:11:10.372056Z",
     "iopub.status.busy": "2025-11-23T18:11:10.371396Z",
     "iopub.status.idle": "2025-11-23T18:11:10.381996Z",
     "shell.execute_reply": "2025-11-23T18:11:10.381184Z",
     "shell.execute_reply.started": "2025-11-23T18:11:10.372024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Цикл оценки модели\n",
    "def evaluate(model, tokenizer, test_dataset, batch_size, k=5):\n",
    "    all_preds = []\n",
    "    all_golds = []\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size), desc='Processing dataset'):\n",
    "        batch = test_dataset[i:i + batch_size]\n",
    "        inputs = preprocess_batch(batch)\n",
    "        input_ids = inputs['input_ids'].to(DEVICE)\n",
    "        attention_mask = inputs['attention_mask'].to(DEVICE)\n",
    "        batch_preds = []\n",
    "        for _ in range(k):\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.95,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            batch_preds.append([d.split(\"<|assistant|>\")[-1].strip() for d in decoded])\n",
    "            \n",
    "        batch_preds = list(map(list, zip(*batch_preds)))\n",
    "        all_preds.extend(batch_preds)\n",
    "        all_golds.extend([s for s in batch['answer']])\n",
    "\n",
    "    # Сохраняем и возвращаем результат\n",
    "    result = {}\n",
    "    result['pass@1'] = pass_at_k(all_preds, all_golds, 1)\n",
    "    result['pass@5'] = pass_at_k(all_preds, all_golds, 5)\n",
    "    result['self_consistency'] = self_consistency(all_preds)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:30:19.711390Z",
     "iopub.status.busy": "2025-11-23T17:30:19.710631Z",
     "iopub.status.idle": "2025-11-23T17:45:29.706014Z",
     "shell.execute_reply": "2025-11-23T17:45:29.704916Z",
     "shell.execute_reply.started": "2025-11-23T17:30:19.711362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Вывод результатов\n",
    "metrics = evaluate(model, tokenizer, data, BATCH_SIZE)\n",
    "print(f'Pass@1: {metrics['pass@1']}')\n",
    "print(f'Pass@5: {metrics['pass@5']}')\n",
    "print(f'Self consistency: {metrics['self_consistency']}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
